name: Deploy Monitoring & Logging

on:
  workflow_run:
    workflows: ["Rollout and Cleanup Deployments"]  
    types:
      - completed

jobs:
  deploy-monitoring-logging:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.6.5
          terraform_wrapper: false

      - name: Extract Terraform outputs
        run: |
          # Change to terraform directory
          cd terraform || { echo "::error::terraform directory not found"; exit 1; }
          
          # Get outputs using direct file writing
          terraform output -raw aks_resource_group > aks_rg.txt
          terraform output -raw aks_cluster_name > aks_cluster.txt
          
          # Read values from files
          AKS_RG=$(cat aks_rg.txt)
          AKS_CLUSTER=$(cat aks_cluster.txt)
          
          # Clean values (remove any debug lines)
          AKS_RG=$(echo "$AKS_RG" | head -n 1 | tr -d '\n')
          AKS_CLUSTER=$(echo "$AKS_CLUSTER" | head -n 1 | tr -d '\n')
          
          # Verify outputs
          if [ -z "$AKS_RG" ] || [ -z "$AKS_CLUSTER" ]; then
            echo "::error::Failed to get Terraform outputs"
            echo "Debug output:"
            cat aks_rg.txt
            cat aks_cluster.txt
            exit 1
          fi
          
          # Set environment variables
          echo "AKS_RG=$AKS_RG" >> $GITHUB_ENV
          echo "AKS_CLUSTER=$AKS_CLUSTER" >> $GITHUB_ENV
       
      - name: Configure AKS credentials
        run: |
          # Get credentials with admin access (recommended for CI/CD)
          az aks get-credentials \
            --resource-group "bud-restauranty-rg" \
            --name "bud-restauranty-cluster" \
            --admin \
            --overwrite-existing


      - name: Add Helm Repos
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Install Prometheus Stack # bundled with grafana and node-exporter
        run: |
          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring --create-namespace

      - name: Expose Grafana via Ingress
        run: |
          kubectl apply -f ./monitoring+logging/grafana-ingress.yml

      - name: Clean up existing Kibana release (if any)
        run: |
          helm uninstall kibana --namespace=logging || true
          kubectl delete serviceaccount pre-install-kibana-kibana -n logging --ignore-not-found
          kubectl delete configmap kibana-kibana-helm-scripts -n logging --ignore-not-found
          kubectl delete role pre-install-kibana-kibana -n logging || echo "Role not found, skipping."


      - name: Deploy ELK Stack
        run: |
          helm repo add elastic https://helm.elastic.co
          helm repo update
          helm upgrade --install elasticsearch elastic/elasticsearch --namespace logging --create-namespace
          helm upgrade --install kibana elastic/kibana --namespace logging
          helm upgrade --install filebeat elastic/filebeat --namespace logging

      - name: Expose Kibana via Ingress
        run: |
          kubectl apply -f ./monitoring+logging/kibana-ingress.yml
